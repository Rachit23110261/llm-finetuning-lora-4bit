{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMiEzjN8GXScv+I1IBYh4JX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rachit23110261/llm-finetuning-lora-4bit/blob/main/DL_H02_23110261.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installed all requirementes\n"
      ],
      "metadata": {
        "id": "OUw0WbFyCzbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets accelerate bitsandbytes peft\n"
      ],
      "metadata": {
        "id": "ROeEUMwl9LxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset from Hugging face datasets Library\n",
        "\n",
        "- Library used -> wikitext 10,000 samples\n",
        "- tokenizer used -> Autotokenizer corresponding to gpt-2\n",
        "- Tokenized the whole dataset with dataset.map() and Tokenizer function\n",
        "- Language models like GPT are usually trained on fixed-length blocks of tokens. Here, we define each block as 128 tokens long.\n",
        "- attention mask initialized with 1"
      ],
      "metadata": {
        "id": "iTuXdXN3DW98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load dataset ( wikipedia dataset: easier to analyze)\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train[:10000]\")\n",
        "\n",
        "# It Loads the tokenizer corresponding to GPT-2 model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # For padding compatibility\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"])\n",
        "\n",
        "# Tokenize dataset (do NOT return special tokens mask)\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Grouping function: split into chunks of 128 tokens\n",
        "block_size = 128\n",
        "\n",
        "def group_texts(examples):\n",
        "    concatenated = sum(examples[\"input_ids\"], [])\n",
        "    total_length = (len(concatenated) // block_size) * block_size\n",
        "    result = {\n",
        "        \"input_ids\": [concatenated[i:i+block_size] for i in range(0, total_length, block_size)],\n",
        "        \"attention_mask\": [[1] * block_size] * (total_length // block_size),\n",
        "    }\n",
        "    return result\n",
        "\n",
        "# Apply chunking\n",
        "lm_dataset = tokenized_dataset.map(group_texts, batched=True, remove_columns=tokenized_dataset.column_names)\n",
        "\n",
        "# Check first sample\n",
        "print(lm_dataset[0])\n"
      ],
      "metadata": {
        "id": "X1JWeqra80Hs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a0652b-5f1f-447d-ac1d-4576906c3286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198, 2311, 73, 13090, 645, 569, 18354, 7496, 513, 1058, 1279, 2954, 29, 17740, 357, 4960, 1058, 10545, 230, 99, 161, 254, 112, 5641, 44444, 9202, 25084, 24440, 12675, 11839, 18, 837, 6578, 764, 569, 18354, 7496, 286, 262, 30193, 513, 1267, 837, 8811, 6412, 284, 355, 569, 18354, 7496, 17740, 6711, 2354, 2869, 837, 318, 257, 16106, 2597, 2488, 12, 31, 2712, 2008, 983, 4166, 416, 29490, 290, 6343, 13, 44206, 329, 262, 14047, 44685, 764, 28728, 287, 3269, 2813, 287, 2869, 837, 340, 318, 262, 2368, 983, 287, 262, 569, 18354, 7496, 2168, 764, 12645, 278, 262, 976, 21748, 286, 16106, 290, 1103, 2488, 12, 31, 640, 11327, 355, 663, 27677, 837, 262, 1621, 4539, 10730, 284, 262], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining pretrained *model*\n",
        "\n",
        "- model used -> \"EleutherAI/gpt-neo-1.3B\"\n",
        "- Loaded model in 4-bit precision to save GPU memory\n",
        "- Added Low rank Adaption (Lora) with r=8\n",
        "- r=8: Controls the compression ratio (low-rank dimension). Smaller r = fewer trainable params.-\n",
        "- Target Modules: Specify which layers in the transformer to modify (usually q/k/v projections).\n",
        "- Task Type=\"CAUSAL_LM\": Sets task type to causal language modeling (next-token prediction)."
      ],
      "metadata": {
        "id": "efLFb-bHP1HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch\n",
        "\n",
        "model_name = \"EleutherAI/gpt-neo-1.3B\"  # ~1.3B params\n",
        "\n",
        "# Load model in 4-bit\n",
        "model = AutoModelForCausalLM.from_pretrained(a\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Prepare for LoRA fine-tuning\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # You can use a broader list depending on the architecture\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Inject LoRA adapters\n",
        "model = get_peft_model(model, lora_config)\n"
      ],
      "metadata": {
        "id": "IJ7YIH2x9ky5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculated Accuracy and perplexity for the pretrained mode on the chosen dataset (Before- Training)\n",
        "\n",
        "- Calculated on small dataset of 512 datapoints\n"
      ],
      "metadata": {
        "id": "VQdj5vehV8Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "import math\n",
        "\n",
        "# Use default data collator for Causal L\n",
        "def compute_perplexity_acc(model, dataset,tokenizer):\n",
        "    # Set model to evaluation mode\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "    # Prepare DataLoader\n",
        "    eval_dataloader = DataLoader(dataset , batch_size=8, collate_fn=data_collator)\n",
        "\n",
        "    # Set model to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Move model to the appropriate device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    # No gradient computation needed\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = input_ids.clone()\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Compute total loss\n",
        "            total_loss += loss.item() * input_ids.numel()\n",
        "            total_tokens += input_ids.numel()\n",
        "\n",
        "            # Compute accuracy: compare logits' argmax with labels\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            correct_predictions += (predictions == labels).float().sum().item()\n",
        "\n",
        "    # Compute perplexity and accuracy\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(avg_loss)\n",
        "    accuracy = correct_predictions / total_tokens\n",
        "    return perplexity, accuracy\n",
        "perplexity, accuracy = compute_perplexity_acc(model, lm_dataset.select(range(512)), tokenizer)\n",
        "print(f\"Perplexity of Pre_Trained Model: {perplexity:.4f}\")\n",
        "print(f\"Accuracy of Pre_trained Model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EijJO5tBuUM",
        "outputId": "e6bf1f29-0350-4cb3-b95d-e417ad52290e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Pre_Trained Model: 38.6291\n",
            "Accuracy of Pre_trained Model: 0.0199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defined training arguments\n",
        "\n",
        "- Number of batches trained on Gpu at once => 50\n",
        "- number of epochs trained => 1\n",
        "- learning rate => 0.0002"
      ],
      "metadata": {
        "id": "rlY68nliWant"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt-neo-lora-finetuned\",\n",
        "    per_device_train_batch_size=50,\n",
        "    per_device_eval_batch_size=50,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    report_to=\"none\",  # avoid using wandb\n",
        "    fp16=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qykxc7--7n7",
        "outputId": "886d3c0f-f81c-4577-8fa4-9dd4db9f4734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trained the model on wiki-text dataset"
      ],
      "metadata": {
        "id": "LLLrH_-nW2uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Data collator pads inputs and creates labels for LM\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_dataset,\n",
        "    eval_dataset=lm_dataset.select(range(512)),  # small subset for eval\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "EP1Ai5OM--lr",
        "outputId": "2bc09b88-43f1-40f0-b329-30409566acda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 08:08, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.050000</td>\n",
              "      <td>3.102378</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=25, training_loss=3.0575904083251952, metrics={'train_runtime': 507.9804, 'train_samples_per_second': 9.861, 'train_steps_per_second': 0.049, 'total_flos': 4646507642880000.0, 'train_loss': 3.0575904083251952, 'epoch': 0.9900990099009901})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saved the trained model"
      ],
      "metadata": {
        "id": "9reJBdKSW-NI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./gpt-neo-lora-finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt-neo-lora-finetuned\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8v8L_sw_fsH",
        "outputId": "4cc43117-6cfa-465e-ea6f-e8eeca624cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gpt-neo-lora-finetuned/tokenizer_config.json',\n",
              " './gpt-neo-lora-finetuned/special_tokens_map.json',\n",
              " './gpt-neo-lora-finetuned/vocab.json',\n",
              " './gpt-neo-lora-finetuned/merges.txt',\n",
              " './gpt-neo-lora-finetuned/added_tokens.json',\n",
              " './gpt-neo-lora-finetuned/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the saved model and calculated perplexity and accuracy for the fine-tuned model"
      ],
      "metadata": {
        "id": "qNjutikMXECH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the model and tokenizer from the saved directory\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./gpt-neo-lora-finetuned\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./gpt-neo-lora-finetuned\")\n",
        "\n",
        "# Move model to CUDA if available\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "JHuJyftRFNRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity, accuracy = compute_perplexity_acc(model, lm_dataset.select(range(512)), tokenizer)\n",
        "print(f\"Perplexity of Fine_tuned Model: {perplexity:.4f}\")\n",
        "print(f\"Accuracy of Fine_tuned Model: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsKLD5pnIm4q",
        "outputId": "352ca720-1682-40fa-b1f7-1026b49ec7f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Fine_tuned Model: 21.5071\n",
            "Accuracy of Fine_tuned Model: 0.0106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qualitative Analysis on fine-tuned model and pre-trained model"
      ],
      "metadata": {
        "id": "DgfiGUJNXPTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from accelerate import init_empty_weights, infer_auto_device_map\n",
        "from transformers import AutoConfig\n",
        "\n",
        "# Create a folder for offloading\n",
        "os.makedirs(\"/content/offload\", exist_ok=True)\n",
        "\n",
        "# Define BitsAndBytes quantization config\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Step 1: Create an empty model to calculate device map\n",
        "config = AutoConfig.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "with init_empty_weights():\n",
        "    empty_model = AutoModelForCausalLM.from_config(config)\n",
        "    empty_model.tie_weights()  # fix: tie weights before infer_auto_device_map\n",
        "\n",
        "device_map = infer_auto_device_map(\n",
        "    empty_model,\n",
        "    max_memory={\n",
        "        0: \"8GiB\",  # Adjust depending on your GPU\n",
        "        \"cpu\": \"30GiB\"\n",
        "    },\n",
        "    no_split_module_classes=[\"GPTNeoBlock\"]\n",
        ")\n",
        "\n",
        "# Step 2: Load the quantized model with device map and offload folder\n",
        "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"EleutherAI/gpt-neo-1.3B\",\n",
        "    device_map=device_map,\n",
        "    quantization_config=quant_config,\n",
        "    offload_folder=\"/content/offload\"\n",
        ")\n",
        "\n",
        "# Step 3: Load LoRA finetuned model similarly\n",
        "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"/content/gpt-neo-lora-finetuned\",\n",
        "    device_map=device_map,\n",
        "    quantization_config=quant_config,\n",
        "    offload_folder=\"/content/offload\"\n",
        ")\n",
        "\n",
        "# Step 4: Create pipelines WITHOUT passing `device=0` (accelerate handles it)\n",
        "generator_pretrained = pipeline(\"text-generation\", model=pretrained_model, tokenizer=tokenizer)\n",
        "generator_finetuned = pipeline(\"text-generation\", model=finetuned_model, tokenizer=tokenizer)\n",
        "\n",
        "# Step 5: Define prompts and generate\n",
        "prompts = [\n",
        "    \"The football world cup should be won by\",\n",
        "    \"We should do\",\n",
        "    \"The Meal should we take is\",\n",
        "    \"A healthy person is a person who\",\n",
        "    \"The best cricket player is\",\n",
        "    \"In a disaster we should\",\n",
        "    \"We should do our assignment on\",\n",
        "    \"The deadline is near I should\",\n",
        "    \"Success comes from\",\n",
        "   \"Taj Mahal was built by\"\n",
        "]\n",
        "\n",
        "outputs_pretrained = [generator_pretrained(prompt, max_length=50, do_sample=True)[0]['generated_text'] for prompt in prompts]\n",
        "outputs_finetuned = [generator_finetuned(prompt, max_length=50, do_sample=True)[0]['generated_text'] for prompt in prompts]\n",
        "\n",
        "# Step 6: Print results\n",
        "print(\"\\n=== COMPARISON ===\")\n",
        "for i, prompt in enumerate(prompts):\n",
        "    print(f\"\\nPrompt {i+1}: {prompt}\")\n",
        "    print(f\"ðŸ”¹ Pretrained: {outputs_pretrained[i]}\")\n",
        "    print(f\"ðŸ”¸ Finetuned : {outputs_finetuned[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOf-hWgtB_Gg",
        "outputId": "65fc1673-0393-4715-d6e4-28f02668b7c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== COMPARISON ===\n",
            "\n",
            "Prompt 1: The football world cup should be won by\n",
            "ðŸ”¹ Pretrained: The football world cup should be won by the Netherlands. The game is a wonderful display of teamwork, skill and creativity. But when you lose to England you should remember the Dutch are a team that never give up.\n",
            "\n",
            "Thereâ€™s more\n",
            "ðŸ”¸ Finetuned : The football world cup should be won by the team with the most points after the quarter-finals. If a team has a better goal difference at the end of the game, they should have a higher chance of winning the cup but it should be treated\n",
            "\n",
            "Prompt 2: We should do\n",
            "ðŸ”¹ Pretrained: We should do that to some extent. I think\n",
            "that's how we should have\n",
            "responded to that.\n",
            "The most important thing we\n",
            "do is, we take out one\n",
            "of the most important\n",
            "areas, one of the most\n",
            "ðŸ”¸ Finetuned : We should do the following :\n",
            "\n",
            "â€¢ We should not use any new or existing funding, such as the UK National Lottery, for a project to benefit any other charities while taking up a new grant;\n",
            "\n",
            "â€¢ We should not use any\n",
            "\n",
            "Prompt 3: The Meal should we take is\n",
            "ðŸ”¹ Pretrained: The Meal should we take is for $16.95, but we were told it was $17.95, a lot of money for one dinner. So, we paid a big deal of money on a reservation, but the whole thing was very\n",
            "ðŸ”¸ Finetuned : The Meal should we take is what to do with the food?\" I asked her, and she nodded at me to come back with the ingredients I wanted in a bowl, and we did. We brought a small bowl of salad, one of the few\n",
            "\n",
            "Prompt 4: A healthy person is a person who\n",
            "ðŸ”¹ Pretrained: A healthy person is a person who enjoys a sense of balance, a harmonious relationship with both the inner and the outer world, and who finds it necessary to practice self-discipline.\n",
            "\n",
            "The human being has reached the highest state of being\n",
            "ðŸ”¸ Finetuned : A healthy person is a person who plays an active lifestyle of exercising and eating a balanced diet. A person is healthy if he or she does not have a heart disease, diabetes, high cholesterol, obesity or other health conditions.\n",
            " \n",
            "A heart\n",
            "\n",
            "Prompt 5: The best cricket player is\n",
            "ðŸ”¹ Pretrained: The best cricket player is in the first 10 overs of a game. For me, that is the position of the best batsman. That's a statement I would make to my own captain during those first 10 overs of a match. It's something\n",
            "ðŸ”¸ Finetuned : The best cricket player is an individual whose game of cricket is recognised to be the most difficult to analyse and analyze, who excels at fielding and is capable of taking control of the game with a solid wristy glove,\" he said. \"He also\n",
            "\n",
            "Prompt 6: In a disaster we should\n",
            "ðŸ”¹ Pretrained: In a disaster we should always try to do what little we can to help others.\n",
            "\n",
            "Iâ€™ve never liked public speaking, so when I learned that I was going to have to give my speech at a small seminar at the university,\n",
            "ðŸ”¸ Finetuned : In a disaster we should be able to predict where it will be blown up and when it will be bombed [by a North Korean missile]. But the North Korean missile test is also a very provocative act because it had a range of more than 2,\n",
            "\n",
            "Prompt 7: We should do our assignment on\n",
            "ðŸ”¹ Pretrained: We should do our assignment on Monday. On Tuesday, we can do the project.\n",
            "\n",
            "â€œWhat I had hoped to do was to work with the committee to find a better way to manage the issue and bring it to the attention of the\n",
            "ðŸ”¸ Finetuned : We should do our assignment on the basis of the \" _A-C_ \" technique. If we are to be \" _A_ \", we require the best answer in three distinct parts. One is that we should be able to recognize the\n",
            "\n",
            "Prompt 8: The deadline is near I should\n",
            "ðŸ”¹ Pretrained: The deadline is near I should say. I had a good long look at the \n",
            "list, but I canâ€™t see anywhere where we have a deal listed. Let me know if \n",
            "you see the deal I need to have it\n",
            "ðŸ”¸ Finetuned : The deadline is near I should have been in front of her by now but I was a little worried about the result of the race. Now that she beat all the other horses, I was in her head and I thought she would be going home if\n",
            "\n",
            "Prompt 9: Success comes from\n",
            "ðŸ”¹ Pretrained: Success comes from the word, which is\n",
            "in the singular and is a\n",
            "conjunctive participle.\n",
            "That means that you use\n",
            "that person to be the object\n",
            "of this and there's no plural form.\n",
            "And the adjective that\n",
            "ðŸ”¸ Finetuned : Success comes from my sense of humor. \n",
            " it is a good feeling to have something to fall back on ; if there were no bad guys, \n",
            " people would be so disappointed that there was no more villain. \n",
            " when I was younger\n",
            "\n",
            "Prompt 10: Taj Mahal was built by\n",
            "ðŸ”¹ Pretrained: Taj Mahal was built by T.C. Krishnan in 1889. It is one of the oldest monuments of Sri Lanka. It is a temple of Hinduism in the Vanni region of the country. It is situated in Anuradh\n",
            "ðŸ”¸ Finetuned : Taj Mahal was built by the Rajputs of Punjab, the 'Raj' dynasty, in the city of Gujrat, Punjab, India. The palace is also called the'Taj Mahal'by locals in Punjab. It\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Human Evaluation Scores:\n",
        "\n",
        "| Prompt | Fluency (Pre) | Fluency (Fine) | Relevance (Pre) | Relevance (Fine) | Correctness (Pre) | Correctness (Fine) |\n",
        "|--------|---------------|----------------|------------------|-------------------|---------------------|----------------------|\n",
        "| The football world cup should be won by | $4$ | $4$ | $2$ | $4$ | $3$ | $4$ |\n",
        "| We should do | $3$ | $4$ | $2$ | $4$ | $2$ | $4$ |\n",
        "| The Meal should we take is | $3$ | $4$ | $2$ | $3$ | $2$ | $3$ |\n",
        "| A healthy person is a person who | $5$ | $5$ | $4$ | $5$ | $4$ | $5$ |\n",
        "| The best cricket player is | $4$ | $4$ | $3$ | $4$ | $3$ | $4$ |\n",
        "| In a disaster we should | $3$ | $3$ | $2$ | $3$ | $2$ | $2$ |\n",
        "| We should do our assignment on | $3$ | $4$ | $3$ | $4$ | $3$ | $4$ |\n",
        "| The deadline is near I should | $4$ | $4$ | $3$ | $3$ | $3$ | $3$ |\n",
        "| Success comes from | $4$ | $4$ | $2$ | $4$ | $2$ | $2$ |\n",
        "| Taj Mahal was built by | $4$ | $4$ | $4$ | $4$ | $1$ | $1$ |\n"
      ],
      "metadata": {
        "id": "69HVLdTAL-WC"
      }
    }
  ]
}